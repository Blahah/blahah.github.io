## How can we measure quality, not privelege?

Evaluating scientists based on publication citations and impact factor have been coming under criticism since [at least 15 years ago](http://www.bmj.com/content/314/7079/497.1), and [frequently since then](https://www.zotero.org/groups/impact_factor_problems/items). [Nothing has changed](http://datapub.cdlib.org/2013/05/22/impact-factors/). As a young scientist collaborating on a variety of projects, I've got a bunch of papers in preparation with colleagues. This has made me acutely aware of the poisonous effect the metrics situation has on science.

My older or more experienced collaborators, almost without exception, are obsessed with publishing in high-impact journals. If it's not a Nature paper, it better be Science. Any 'lower' than that and we've essentially failed. [REF assessments](http://www.ref.ac.uk/) are coming up, and we need papers in good journals or we've got no chance or progressing in our careers. Notice that we don't need *good papers*, we need them in *good journals*. We have to demonstrate **impact**. Whatever that is.

General interest journals like Nature and Science have some of the most overblown, sexed-up papers I've seen anywhere. By contrast, journals like PLOS tend, in my experience, to have interesting and thoughtful papers with claims that more closely match the results. [ArXiv](http://arxiv.org) (not a peer reviewed journal) even more so. This is reflected in the relative retraction rates - [Impact Factor is correlated with retractions](http://iai.asm.org/content/79/10/3855.full).

Being in Cambridge, and being a white male, I'm always trying to be aware of my privelege. This situation is a grotesque example of it. People at institutions like this one are very successful when measured by how often they publish in glamour journals. That is at least in part because *everyone* here routinely submits to them. Written a paper? Send it to Nature first. If it gets rejected, climb gradually down the prestige ladder (Science > Cell > Current Biology) until it gets accepted. Contrast to my previous university, the University of the West of England, where most researchers would never even think of sending their work to Nature, no matter how cool it was.

We are constantly gaming the metrics system, whether we're conscious of it or not. That's just how metrics work - you measure people on something and they will optimise for it. This is famously a problem in software development, where programmers can be measured by the number of lines of code they produce, which incentivises writing less concise code.

> "Measuring software productivity by lines of code is like measuring progress on an airplane by how much it weighs."
> _- Bill Gates_

For example, Cambridge researchers tend to cite other people who have passed through similarly priveleged institutions\*. Maybe this is deliberate, but maybe it's because they're just living in a bubble. It's a bunch of (mostly) rich white men circle-citing one another. This citation club is a system for concentrating and maintaining privelege. So when we measure scientists on citations and impact, we're just measuring their privelege.

_*this is anecdotal_

This is a widely recognised problem, and many people are making an effort to break away from these crappy measures of science output to less power-concentrated ones. Movements like [altmetrics](http://altmetrics.org/manifesto) have risen, exemplified by [AltMetric](http://altmetric.com) and [ImpactStory](http://impactstory.org), measuring 'impact' across social media and the web.

These sites do something truly useful - measure community engagement. If we think science is important, then engaging the scientific community and the public is also important. But is our ability to do this a measure of quality science? In other words, should be be evaluating scientists based on these metrics? On their ability to self-promote? I think not.

People who exercise influence and privelege in traditional avenues like journal publishing are more likely to score highly on traditional metrics, and people who exercise their influence and privelege in new media will be successful when measured by altmetrics.

A recent [AltMetrics blog post about gaming AltMetrics](http://www.altmetric.com/blog/gaming-altmetrics/) lists the ways people might try to get a higher AltMetrics score:

> 1. Alice has a new paper out. She tweets about it. HootSuite automatically posts all of her tweets to Facebook and Google+.
> 2. Alice has a new paper out. She writes about it on her lab’s blog and sends an email highlighting it to a colleague who reviews for Faculty of 1000.
> 3. Alice has a new paper out. She asks her colleagues to share it via social media if they think it’d be useful to others.
> 4. Alice has a new paper out. She asks those grad students of hers who blog to write about it.
> 5. Alice has a new paper out. She believes that it contains important information for diabetes patients and so pays for an in-stream advert on Twitter.
> 6. Alice has a new paper out. She believes that it contains important information for diabetes patients and so signs up to a ’100 retweets for $$$’ service.

They consider all of these acceptable except number 6.

I don't. Why should financial privelege be treated differently from the social privelege of a researcher who asks a colleague to review for F1000? Or asks a friend with a load of twitter followers to retweet? I think I would only feel really comfortable doing #1, and might force myself to do #2. It's OK if people do them all, just don't reward them for it with Internet Points and then use those points to conduct professional assessments of anything except what they actually measure.

Privelege in general is a problem, and metrics of any kind that don't directly measure the quality of the ideas and execution are perpetuating privelege. I'm not at all convinced that currently popular altmetrics will help with this. They should help with other problems. For example, the pace of progress should be accelerated by breaking free of the painfully slow traditional review cycle.

Importantly, altmetrics diffuse privelege; different people come to hold it. The divisions of privelege might cease to be drawn along gender, race and wealth lines. This is a very good thing - if privelege is something people can earn rather than being born with it, that's a big chunk of the problem dealth with.

But I'm talking about how we measure the quality of scientists and their outputs, how we make hiring decisions and decide what to read and cite. And for this purpose, 'impact' is a bad fit.

What we need is something that meets these conditions:

- focussed on quality
- objective
- difficult or impossible to game
- enables scientists and hiring committees to judge the work without having to read every single paper

Here's one idea: [automated sentiment analysis](http://en.wikipedia.org/wiki/Sentiment_analysis) across the post-publication review ecosystem. A set of metrics for every publication that measures *how experts feel about it*. Not just the experts we invite to the party, like at [F1000 Prime](http://f1000.com/prime), but openly, across all the places where experts talk about science. Surely that's a better measure of quality than anything we're doing now?

The basis for this is already in place. [PubPeer](https://pubpeer.com/) and [PubMed Commons](http://www.ncbi.nlm.nih.gov/pubmedcommons/) are crowd-sourcing expert evaluations of science. PubPeer is excellent because they verify the identity of commenters, but keep the comments anonymous. This means young researchers like me can comment honestly with no fear of career reprisals. PubMed Commons takes the alternative route of signed reviews - also valuable for preventing unabated negativity. If we could assess sentiment across many such post-publication peer-review sites, that might be a relatively objective measure of quality.

This idea has flaws, sure, but I think it's heading in the right direction, because it's *really* hard to game it. You can't control what other people say. Maybe you could try to convince all your colleagues to write glowing reviews - but this could be tackled, for example by giving weightings to reviewers according to their distance from the author in the [collaboration graph](http://en.wikipedia.org/wiki/Collaboration_graph).

In summary: Privelege is bad, quality is good. Self-promotion not the same as quality. How can we measure quality? Maybe with algorithms that collect expert opinion.

I'd love to hear what you think in the comments below, or on [Twitter](http://twitter.com/blahah404).
